{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5eec64ff-fbd2-43a4-839b-4684b2091bab",
   "metadata": {},
   "source": [
    "## How to Set the File Path and Page Range for Analysis\n",
    "\n",
    "This notebook is designed so that **only one small section ever needs to be edited** when analyzing a new document or a different part of the same document. All other phases should remain unchanged.\n",
    "\n",
    "Follow the steps below **before running any other cells**.\n",
    "\n",
    "---\n",
    "\n",
    "### Step 1: Locate **PHASE 0 — USER-DEFINED CONFIGURATION**\n",
    "\n",
    "At the very top of the notebook, you will see a clearly labeled code block:\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43baa71e-9afc-46af-83cc-d4822ccbacba",
   "metadata": {},
   "source": [
    "\n",
    "This is the **only place** where you should declare:\n",
    "- The document being analyzed\n",
    "- The page numbers to be analyzed\n",
    "- The section label used for traceability\n",
    "\n",
    "Do **not** edit any other phases unless you are intentionally changing methodology.\n",
    "\n",
    "---\n",
    "\n",
    "### Step 2: Declare the PDF File Path\n",
    "\n",
    "Inside **PHASE 0**, find the line:\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58f478f2-d4a0-48b3-b6d4-e79f8060a1a1",
   "metadata": {},
   "source": [
    "\n",
    "Replace `\"input_document.pdf\"` with the path to your file.\n",
    "\n",
    "Examples:\n",
    "\n",
    "- File in the same folder as the notebook:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01fa6c17-1da2-4855-920b-e3c27af25db7",
   "metadata": {},
   "source": [
    "\n",
    "- File in a subfolder:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3eb84471-cb06-46c9-ac13-5564e43c57b3",
   "metadata": {},
   "source": [
    "\n",
    "- Absolute path:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c94c3b4-0750-4ff3-8fb8-572cf4182019",
   "metadata": {},
   "source": [
    "\n",
    "The notebook will not search for files automatically. The path must be explicit.\n",
    "\n",
    "---\n",
    "\n",
    "### Step 3: Declare the Page Range to Be Analyzed\n",
    "\n",
    "Still within **PHASE 0**, locate the page range variables:\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b10c1cd-9b7f-45b4-b914-d0a49e8cb10c",
   "metadata": {},
   "source": [
    "\n",
    "Update these values to match the pages you want analyzed.\n",
    "\n",
    "Important notes:\n",
    "- Page numbers refer to **PDF page numbers**, not printed chapter numbers.\n",
    "- The range is **inclusive**. Both the start and end pages will be analyzed.\n",
    "- Pages outside this range will never be read or processed.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d2b4c88-c536-4592-bc6d-eebef0f195c7",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "### Step 4: Declare a Section Label (Required)\n",
    "\n",
    "The section label is used to tag every extracted claim for traceability.\n",
    "\n",
    "Find the line: \"PDF_PATH = \"input_document.pdf\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c23b5c17-b00a-436e-98d5-2979e6fb4511",
   "metadata": {},
   "source": [
    "\n",
    "Replace the text with a meaningful label (use \"\") for the portion of the document being analyzed."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d27dc0a6-cab9-4c89-8e7a-b125e404828d",
   "metadata": {},
   "source": [
    "\n",
    "This label will appear in the claims table and downstream exports.\n",
    "\n",
    "---\n",
    "\n",
    "### Step 5: Save and Run the Notebook from the Top\n",
    "\n",
    "After updating **PHASE 0**:\n",
    "\n",
    "1. Save the notebook\n",
    "2. Restart the kernel (recommended)\n",
    "3. Run all cells from top to bottom\n",
    "\n",
    "This ensures:\n",
    "- No stale variables remain in memory\n",
    "- Page scoping is enforced correctly\n",
    "- Claims are extracted only from the declared range\n",
    "\n",
    "---\n",
    "\n",
    "### Important Methodological Reminder\n",
    "\n",
    "Do not move, duplicate, or redefine the configuration variables elsewhere in the notebook.\n",
    "\n",
    "All downstream phases **assume** that:\n",
    "- The file path was declared once\n",
    "- The page range was declared once\n",
    "- The scope of analysis is fixed before extraction begins\n",
    "\n",
    "This design choice is intentional and supports reproducibility, auditability, and peer review.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b48f6b8c-67dc-4d8f-9fbb-16efe94e19b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =====================================\n",
    "# PHASE 0 — USER-DEFINED CONFIGURATION\n",
    "# =====================================\n",
    "\n",
    "PDF_PATH = \n",
    "\n",
    "PAGE_RANGE_START = \n",
    "PAGE_RANGE_END   = \n",
    "\n",
    "SECTION_LABEL = \n",
    "\n",
    "EXCLUSION_KEYWORDS = [\n",
    "    \"notes\",\n",
    "    \"endnotes\",\n",
    "    \"footnotes\",\n",
    "    \"references\",\n",
    "    \"bibliography\",\n",
    "    \"works cited\",\n",
    "    \"acknowledgements\"\n",
    "]\n",
    "\n",
    "MIN_SENTENCE_LENGTH = 30\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "83f489f1-db69-4fa3-95cf-93ae1984be82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =====================================\n",
    "# PHASE 1 — PAGE-SCOPED TEXT EXTRACTION\n",
    "# =====================================\n",
    "\n",
    "from PyPDF2 import PdfReader\n",
    "\n",
    "reader = PdfReader(PDF_PATH)\n",
    "\n",
    "page_text_blocks = []\n",
    "\n",
    "for page_index in range(PAGE_RANGE_START - 1, PAGE_RANGE_END):\n",
    "    page = reader.pages[page_index]\n",
    "    text = page.extract_text()\n",
    "    \n",
    "    # TIGHTENED: Explicitly check that text is not None AND not empty after stripping\n",
    "    # This ensures we capture any page with actual content, even sparse pages\n",
    "    if text and text.strip():\n",
    "        page_text_blocks.append({\n",
    "            \"page\": page_index + 1,\n",
    "            \"raw_text\": text\n",
    "        })\n",
    "\n",
    "if not page_text_blocks:\n",
    "    raise ValueError(\"No text extracted from specified page range.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "631590c4-c078-457b-a927-1ca51dac28f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =====================================\n",
    "# PHASE 2 — TEXT NORMALIZATION\n",
    "# =====================================\n",
    "\n",
    "import re\n",
    "\n",
    "def normalize_text(text):\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    return text.strip()\n",
    "\n",
    "for block in page_text_blocks:\n",
    "    block[\"normalized_text\"] = normalize_text(block[\"raw_text\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "be684d1f-1881-4787-ab86-4f10fa44cc7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =====================================\n",
    "# PHASE 3 — EXCLUSION SECTION DETECTION\n",
    "# =====================================\n",
    "\n",
    "def detect_excluded_section(text, keywords):\n",
    "    text_lower = text.lower()\n",
    "    return any(keyword in text_lower for keyword in keywords)\n",
    "\n",
    "for block in page_text_blocks:\n",
    "    block[\"is_excluded\"] = detect_excluded_section(\n",
    "        block[\"normalized_text\"],\n",
    "        EXCLUSION_KEYWORDS\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d1fd8d12-473b-43d3-a3a1-abbc55527049",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =====================================\n",
    "# PHASE 4 — SENTENCE-LEVEL CLAIM EXTRACTION\n",
    "# =====================================\n",
    "\n",
    "import uuid\n",
    "import re\n",
    "\n",
    "def is_visual_content(sentence):\n",
    "    \"\"\"\n",
    "    Detect and exclude visual content markers:\n",
    "    - Table indicators (Table N:, borders, pipes)\n",
    "    - Chart/Graph indicators (Chart N:, Figure N:, percentage patterns)\n",
    "    - Special characters associated with visual content\n",
    "    - Lines that are primarily numbers or percentages\n",
    "    \"\"\"\n",
    "    text = sentence.strip().lower()\n",
    "    \n",
    "    # PATTERN 1: Visual element labels\n",
    "    visual_labels = [\n",
    "        r'^\\s*(table|tbl)\\.?\\s+\\d+:',      # Table 1:, Table 1., Tbl 1:\n",
    "        r'^\\s*(figure|fig)\\.?\\s+\\d+:',     # Figure 1:, Fig 1:\n",
    "        r'^\\s*(chart|graph|diagram)\\.?\\s+\\d+:', # Chart 1:, etc.\n",
    "        r'^\\s*(exhibit|appendix)\\.?\\s+[a-z0-9]:',  # Exhibit A:, etc.\n",
    "        r'^\\s*source:',                      # Source: [attribution]\n",
    "    ]\n",
    "    \n",
    "    for pattern in visual_labels:\n",
    "        if re.search(pattern, text):\n",
    "            return True\n",
    "    \n",
    "    # PATTERN 2: Table formatting characters\n",
    "    # Common table border/structure characters\n",
    "    table_chars = ['│', '┤', '├', '┼', '╞', '╡', '═', '║', '┌', '┐', '└', '┘', '─', '┬', '┴']\n",
    "    if any(char in sentence for char in table_chars):\n",
    "        return True\n",
    "    \n",
    "    # PATTERN 3: Heavy use of pipes (column separators)\n",
    "    if sentence.count('|') >= 2:\n",
    "        return True\n",
    "    \n",
    "    # PATTERN 4: Percentage-heavy content (likely chart axis labels)\n",
    "    # Lines with 3+ percentage signs and very few words\n",
    "    percent_count = sentence.count('%')\n",
    "    word_count = len(sentence.split())\n",
    "    if percent_count >= 3 and word_count < 15:\n",
    "        return True\n",
    "    \n",
    "    # PATTERN 5: Legend-style content\n",
    "    # Patterns like \"■ Category A  ■ Category B\"\n",
    "    legend_chars = ['■', '●', '▲', '▼', '◆', '★', '□', '○', '△', '▽', '◇', '☆', '█', '▓', '▒']\n",
    "    if any(char in sentence for char in legend_chars):\n",
    "        return True\n",
    "    \n",
    "    # PATTERN 6: All caps labels with few words (often table headers)\n",
    "    words = sentence.split()\n",
    "    if len(words) <= 5 and sentence.isupper() and len(sentence) > 3:\n",
    "        # But allow common uppercase phrases like acronyms in normal text\n",
    "        # Only exclude if it's VERY short and all caps\n",
    "        if len(words) <= 3:\n",
    "            return True\n",
    "    \n",
    "    return False\n",
    "\n",
    "claims = []\n",
    "\n",
    "for block in page_text_blocks:\n",
    "    if block[\"is_excluded\"]:\n",
    "        continue\n",
    "\n",
    "    sentences = re.split(\n",
    "        r'(?<=[.!?])\\s+',\n",
    "        block[\"normalized_text\"]\n",
    "    )\n",
    "\n",
    "    for sentence in sentences:\n",
    "        sentence = sentence.strip()\n",
    "\n",
    "        # Check minimum length\n",
    "        if len(sentence) < MIN_SENTENCE_LENGTH:\n",
    "            continue\n",
    "        \n",
    "        # Check for visual content\n",
    "        if is_visual_content(sentence):\n",
    "            continue\n",
    "\n",
    "        claims.append({\n",
    "            \"claim_id\": f\"C{str(uuid.uuid4())[:8]}\",\n",
    "            \"claim_text\": sentence,\n",
    "            \"page\": block[\"page\"],\n",
    "            \"section\": SECTION_LABEL,\n",
    "            \"excluded\": False\n",
    "        })\n",
    "\n",
    "if not claims:\n",
    "    raise ValueError(\"No claims extracted. Review page range or filters.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e29bd025-e4b1-4b47-9183-95f2ee63c886",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =====================================\n",
    "# PHASE 5 — CLAIMS TABLE DECLARATION\n",
    "# =====================================\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "PHASE5_OUTPUT = pd.DataFrame(claims)\n",
    "\n",
    "df_claims = PHASE5_OUTPUT.copy()\n",
    "\n",
    "if \"claim_text\" not in df_claims.columns:\n",
    "    raise RuntimeError(\"Claim text column missing from claims table.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6eb70be8-27e7-44f2-a56a-1110d40c6a04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =====================================\n",
    "# PHASE 6 — EXCLUSION AUDIT LOG\n",
    "# =====================================\n",
    "\n",
    "exclusion_log = []\n",
    "\n",
    "for block in page_text_blocks:\n",
    "    if block[\"is_excluded\"]:\n",
    "        matched = [\n",
    "            k for k in EXCLUSION_KEYWORDS\n",
    "            if k in block[\"normalized_text\"].lower()\n",
    "        ]\n",
    "\n",
    "        exclusion_log.append({\n",
    "            \"page\": block[\"page\"],\n",
    "            \"matched_keywords\": matched,\n",
    "            \"section\": SECTION_LABEL\n",
    "        })\n",
    "\n",
    "PHASE6_EXCLUSIONS = pd.DataFrame(exclusion_log)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fb489205-9124-4195-ac1b-feea96894ebe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =====================================\n",
    "# PHASE 7 — INITIAL CLAIM CLASSIFICATION\n",
    "# =====================================\n",
    "\n",
    "def classify_claim_type(text):\n",
    "    t = text.lower()\n",
    "\n",
    "    if any(w in t for w in [\"should\", \"must\", \"ought\", \"policy\", \"recommend\"]):\n",
    "        return \"Policy-prescriptive\"\n",
    "\n",
    "    if any(w in t for w in [\"will\", \"would\", \"expected to\", \"likely\"]):\n",
    "        return \"Predictive\"\n",
    "\n",
    "    if any(w in t for w in [\"because\", \"leads to\", \"results in\", \"causes\"]):\n",
    "        return \"Causal\"\n",
    "\n",
    "    return \"Descriptive\"\n",
    "\n",
    "\n",
    "# Materialize claim_type (non-optional)\n",
    "df_claims[\"claim_type\"] = df_claims[\"claim_text\"].apply(classify_claim_type)\n",
    "\n",
    "# Enforce total classification (Appendix A.4)\n",
    "if df_claims[\"claim_type\"].isnull().any():\n",
    "    raise RuntimeError(\n",
    "        \"Phase 7 failed: claim_type contains null values. \"\n",
    "        \"All claims must receive a classification or be marked 'Unspecified'.\"\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "aef477c1-10f3-43ce-841c-987171e15763",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =====================================\n",
    "# PHASE 7.5 — DISAMBIGUATION & AMBIGUITY FLAGGING\n",
    "# =====================================\n",
    "\n",
    "def detect_structural_signals(text):\n",
    "    t = text.lower()\n",
    "\n",
    "    signals = {\n",
    "        \"normative\": any(w in t for w in [\"should\", \"must\", \"ought\", \"important\"]),\n",
    "        \"policy\": any(w in t for w in [\"policy\", \"regulation\", \"law\"]),\n",
    "        \"causal\": any(w in t for w in [\"because\", \"leads to\", \"results in\", \"causes\"]),\n",
    "        \"predictive\": any(w in t for w in [\"will\", \"would\", \"likely\", \"expected to\"]),\n",
    "        \"descriptive\": any(w in t for w in [\"is\", \"are\", \"was\", \"were\"])\n",
    "    }\n",
    "\n",
    "    return [k for k, v in signals.items() if v]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7c517bc6-3245-4a54-b226-a1307c326a0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =====================================\n",
    "# PHASE 8 — FALSIFIABILITY ASSESSMENT\n",
    "# =====================================\n",
    "\n",
    "def assess_falsifiability(claim_text):\n",
    "    text = claim_text.lower()\n",
    "\n",
    "    if any(term in text for term in [\n",
    "        \"should\", \"must\", \"ought\", \"we believe\", \"it is important\"\n",
    "    ]):\n",
    "        return \"Not falsifiable\"\n",
    "\n",
    "    if any(term in text for term in [\n",
    "        \"may\", \"could\", \"likely\", \"appears to\", \"suggests\"\n",
    "    ]):\n",
    "        return \"Weakly falsifiable\"\n",
    "\n",
    "    return \"Falsifiable\"\n",
    "\n",
    "\n",
    "df_claims[\"falsifiability\"] = df_claims[\"claim_text\"].apply(\n",
    "    assess_falsifiability\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7f8b9a74-b170-4360-9603-46dd2d80081f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =====================================\n",
    "# PHASE 8.1 — EVIDENCE TYPOLOGY\n",
    "# =====================================\n",
    "\n",
    "def assign_evidence_type(row):\n",
    "    if row[\"falsifiability\"] == \"Not falsifiable\":\n",
    "        return None  # intentional null\n",
    "\n",
    "    text = row[\"claim_text\"].lower()\n",
    "\n",
    "    if any(term in text for term in [\"data\", \"statistics\", \"rates\", \"percent\"]):\n",
    "        return \"Statistical\"\n",
    "\n",
    "    if any(term in text for term in [\"report\", \"document\", \"record\"]):\n",
    "        return \"Documentary\"\n",
    "\n",
    "    if any(term in text for term in [\"over time\", \"trend\", \"longitudinal\"]):\n",
    "        return \"Longitudinal\"\n",
    "\n",
    "    return \"Unspecified\"\n",
    "\n",
    "\n",
    "df_claims[\"evidence_type\"] = df_claims.apply(\n",
    "    assign_evidence_type,\n",
    "    axis=1\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8564fb0d-b23a-408e-a658-b02800121f84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =====================================\n",
    "# PHASE 8.2 — REGULATORY RELEVANCE FLAG\n",
    "# =====================================\n",
    "\n",
    "def regulatory_relevance(claim_text):\n",
    "    text = claim_text.lower()\n",
    "\n",
    "    if any(term in text for term in [\n",
    "        \"law\", \"regulation\", \"policy\", \"compliance\", \"agency\"\n",
    "    ]):\n",
    "        return True\n",
    "\n",
    "    return False\n",
    "\n",
    "\n",
    "df_claims[\"regulatory_relevance\"] = df_claims[\"claim_text\"].apply(\n",
    "    regulatory_relevance\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c1a1c7e3-4377-4fba-9242-9e534ddb90f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =====================================\n",
    "# PHASE 9 — QUALITY CONTROL CHECKS\n",
    "# =====================================\n",
    "\n",
    "required_columns = [\n",
    "    \"claim_id\",\n",
    "    \"claim_text\",\n",
    "    \"section\",\n",
    "    \"page\",\n",
    "    \"claim_type\",\n",
    "    \"falsifiability\",\n",
    "    \"evidence_type\",\n",
    "    \"regulatory_relevance\"\n",
    "]\n",
    "\n",
    "missing = [c for c in required_columns if c not in df_claims.columns]\n",
    "if missing:\n",
    "    raise RuntimeError(f\"Missing required columns: {missing}\")\n",
    "\n",
    "# Confirm intentional nulls are limited to allowed fields\n",
    "allowed_null_fields = [\"evidence_type\"]\n",
    "\n",
    "for col in df_claims.columns:\n",
    "    if col not in allowed_null_fields:\n",
    "        assert not df_claims[col].isnull().any(), f\"Unexpected nulls in {col}\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3b7bf744-85f7-4977-b2af-7b6bba8bf249",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =====================================\n",
    "# PHASE 10 — FINAL DATASET ASSEMBLY\n",
    "# =====================================\n",
    "\n",
    "FINAL_SCHEMA = [\n",
    "    \"claim_id\",\n",
    "    \"claim_text\",\n",
    "    \"section\",\n",
    "    \"page\",\n",
    "    \"claim_type\",\n",
    "    \"falsifiability\",\n",
    "    \"evidence_type\",\n",
    "    \"regulatory_relevance\"\n",
    "]\n",
    "\n",
    "FINAL_DATASET = df_claims[FINAL_SCHEMA].copy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "42ce3f3f-38fa-4d4d-9ab0-7b3bc03b794f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =====================================\n",
    "# PHASE 10.7 — INTENTIONAL NULL DISCLOSURE\n",
    "# =====================================\n",
    "\n",
    "INTENTIONAL_NULL_FIELDS = {\n",
    "    \"evidence_type\": \"Not applicable or not responsibly assignable\"\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5513880a-3f9c-46a1-a30b-b29ba876d03e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =====================================\n",
    "# PHASE 11 — DATASET HASHING\n",
    "# =====================================\n",
    "\n",
    "import hashlib\n",
    "from datetime import datetime, timezone\n",
    "\n",
    "dataset_json = FINAL_DATASET.to_json(\n",
    "    orient=\"records\"\n",
    ")\n",
    "\n",
    "DATASET_HASH = hashlib.sha256(\n",
    "    dataset_json.encode(\"utf-8\")\n",
    ").hexdigest()\n",
    "\n",
    "LOCK_TIMESTAMP = datetime.now(timezone.utc).isoformat()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6b0bf72f-db33-4152-a240-b7d43e41f88e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =====================================\n",
    "# PHASE 12 — DATASET EXPORT\n",
    "# =====================================\n",
    "\n",
    "FINAL_DATASET.to_csv(\"final_claims_dataset.csv\", index=False)\n",
    "FINAL_DATASET.to_json(\"final_claims_dataset.json\", orient=\"records\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "50a7af93-bb40-48f2-9664-ce783cdb6cb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =====================================\n",
    "# PHASE 13 — EXPORT METADATA\n",
    "# =====================================\n",
    "\n",
    "EXPORT_METADATA = {\n",
    "    \"hash\": DATASET_HASH,\n",
    "    \"locked_at\": LOCK_TIMESTAMP,\n",
    "    \"source_file\": PDF_PATH,\n",
    "    \"page_range\": f\"{PAGE_RANGE_START}-{PAGE_RANGE_END}\",\n",
    "    \"section\": SECTION_LABEL,\n",
    "    \"intentional_nulls\": INTENTIONAL_NULL_FIELDS\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c5a9d11d-9a5a-4b1c-8229-4d4f34af9087",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "claim_type\n",
       "Descriptive            645\n",
       "Policy-prescriptive    118\n",
       "Causal                  15\n",
       "Predictive              11\n",
       "Name: count, dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "falsifiability\n",
       "Falsifiable           689\n",
       "Not falsifiable        56\n",
       "Weakly falsifiable     44\n",
       "Name: count, dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# =====================================\n",
    "# PHASE 14 — RESULTS SUMMARY\n",
    "# =====================================\n",
    "\n",
    "claim_type_counts = FINAL_DATASET[\"claim_type\"].value_counts()\n",
    "falsifiability_counts = FINAL_DATASET[\"falsifiability\"].value_counts()\n",
    "\n",
    "display(claim_type_counts)\n",
    "display(falsifiability_counts)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e92be9c-3af5-43a5-9c8c-9d051b37ffa5",
   "metadata": {},
   "source": [
    "# --------------------------------------\n",
    "# The table above is a representative table based \n",
    "# on the data entered in Phase 0 - User - Defined Configuration\n",
    "# --------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "97484969-8e74-4181-a0d2-04d7bcfcbb57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =====================================\n",
    "# PHASE 14.7 — REVIEWER NOTES\n",
    "# =====================================\n",
    "\n",
    "REVIEWER_NOTES = \"\"\"\n",
    "This dataset reflects sentence-level structural analysis only.\n",
    "No claim has been modified, paraphrased, or evaluated for truth.\n",
    "Intentional nulls are documented and disclosed per Appendix C.6.\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4ba95425-c7ac-4b7e-a8eb-40d26fb5e910",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Phase 15 complete: Word frequency integrated (coordination conjunctions excluded).\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>term</th>\n",
       "      <th>count</th>\n",
       "      <th>analysis_phase</th>\n",
       "      <th>analysis_type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>219</th>\n",
       "      <td>foods</td>\n",
       "      <td>207</td>\n",
       "      <td>Phase_15</td>\n",
       "      <td>word_frequency</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>dietary</td>\n",
       "      <td>186</td>\n",
       "      <td>Phase_15</td>\n",
       "      <td>word_frequency</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>that</td>\n",
       "      <td>150</td>\n",
       "      <td>Phase_15</td>\n",
       "      <td>word_frequency</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>432</th>\n",
       "      <td>is</td>\n",
       "      <td>135</td>\n",
       "      <td>Phase_15</td>\n",
       "      <td>word_frequency</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>211</th>\n",
       "      <td>processed</td>\n",
       "      <td>132</td>\n",
       "      <td>Phase_15</td>\n",
       "      <td>word_frequency</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>192</th>\n",
       "      <td>are</td>\n",
       "      <td>130</td>\n",
       "      <td>Phase_15</td>\n",
       "      <td>word_frequency</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>177</th>\n",
       "      <td>as</td>\n",
       "      <td>129</td>\n",
       "      <td>Phase_15</td>\n",
       "      <td>word_frequency</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>evidence</td>\n",
       "      <td>117</td>\n",
       "      <td>Phase_15</td>\n",
       "      <td>word_frequency</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>health</td>\n",
       "      <td>115</td>\n",
       "      <td>Phase_15</td>\n",
       "      <td>word_frequency</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>guidelines</td>\n",
       "      <td>110</td>\n",
       "      <td>Phase_15</td>\n",
       "      <td>word_frequency</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>238</th>\n",
       "      <td>fat</td>\n",
       "      <td>101</td>\n",
       "      <td>Phase_15</td>\n",
       "      <td>word_frequency</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>americans</td>\n",
       "      <td>92</td>\n",
       "      <td>Phase_15</td>\n",
       "      <td>word_frequency</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>scientific</td>\n",
       "      <td>89</td>\n",
       "      <td>Phase_15</td>\n",
       "      <td>word_frequency</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2025</td>\n",
       "      <td>86</td>\n",
       "      <td>Phase_15</td>\n",
       "      <td>word_frequency</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>270</th>\n",
       "      <td>acid</td>\n",
       "      <td>84</td>\n",
       "      <td>Phase_15</td>\n",
       "      <td>word_frequency</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>foundation</td>\n",
       "      <td>79</td>\n",
       "      <td>Phase_15</td>\n",
       "      <td>word_frequency</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2030</td>\n",
       "      <td>77</td>\n",
       "      <td>Phase_15</td>\n",
       "      <td>word_frequency</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>277</th>\n",
       "      <td>nutrient</td>\n",
       "      <td>77</td>\n",
       "      <td>Phase_15</td>\n",
       "      <td>word_frequency</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1047</th>\n",
       "      <td>highly</td>\n",
       "      <td>73</td>\n",
       "      <td>Phase_15</td>\n",
       "      <td>word_frequency</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>216</th>\n",
       "      <td>food</td>\n",
       "      <td>73</td>\n",
       "      <td>Phase_15</td>\n",
       "      <td>word_frequency</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            term  count analysis_phase   analysis_type\n",
       "219        foods    207       Phase_15  word_frequency\n",
       "2        dietary    186       Phase_15  word_frequency\n",
       "36          that    150       Phase_15  word_frequency\n",
       "432           is    135       Phase_15  word_frequency\n",
       "211    processed    132       Phase_15  word_frequency\n",
       "192          are    130       Phase_15  word_frequency\n",
       "177           as    129       Phase_15  word_frequency\n",
       "16      evidence    117       Phase_15  word_frequency\n",
       "31        health    115       Phase_15  word_frequency\n",
       "3     guidelines    110       Phase_15  word_frequency\n",
       "238          fat    101       Phase_15  word_frequency\n",
       "4      americans     92       Phase_15  word_frequency\n",
       "0     scientific     89       Phase_15  word_frequency\n",
       "5           2025     86       Phase_15  word_frequency\n",
       "270         acid     84       Phase_15  word_frequency\n",
       "1     foundation     79       Phase_15  word_frequency\n",
       "6           2030     77       Phase_15  word_frequency\n",
       "277     nutrient     77       Phase_15  word_frequency\n",
       "1047      highly     73       Phase_15  word_frequency\n",
       "216         food     73       Phase_15  word_frequency"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ==================================================\n",
    "# Phase 15 — Lexical Frequency (Stopword-Controlled)\n",
    "# ==================================================\n",
    "\n",
    "import re\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "\n",
    "# --- INPUT ASSERTION ---\n",
    "if 'df_claims' not in globals():\n",
    "    raise NameError(\"df_claims not found. Ensure Phase 1–7 completed successfully.\")\n",
    "\n",
    "if 'FINAL_DATASET' not in globals():\n",
    "    raise NameError(\"FINAL_DATASET not found. Initialize before Phase 15.\")\n",
    "\n",
    "TEXT_COLUMN = \"claim_text\"\n",
    "\n",
    "if TEXT_COLUMN not in df_claims.columns:\n",
    "    print(f\"Available columns: {df_claims.columns.tolist()}\")\n",
    "    raise KeyError(f\"Column '{TEXT_COLUMN}' not found in df_claims.\")\n",
    "\n",
    "\n",
    "# --- STOPWORDS: Articles, Prepositions, and Coordination Conjunctions ---\n",
    "# (Project 2025 Controlled List)\n",
    "STOP_WORDS = {\n",
    "    # --- Articles ---\n",
    "    \"a\",\"an\",\"the\",\n",
    "    \n",
    "    # --- Prepositions ---\n",
    "    \"about\",\"above\",\"across\",\"after\",\"against\",\"along\",\n",
    "    \"among\",\"around\",\"at\",\"before\",\"behind\",\"below\",\n",
    "    \"beneath\",\"beside\",\"between\",\"beyond\",\"by\",\"concerning\",\n",
    "    \"considering\",\"despite\",\"down\",\"during\",\"except\",\n",
    "    \"for\",\"from\",\"in\",\"inside\",\"into\",\"like\",\"near\",\n",
    "    \"of\",\"off\",\"on\",\"onto\",\"out\",\"outside\",\"over\",\n",
    "    \"past\",\"regarding\",\"round\",\"since\",\"through\",\n",
    "    \"throughout\",\"to\",\"toward\",\"under\",\"underneath\",\n",
    "    \"until\",\"unto\",\"up\",\"upon\",\"with\",\"within\",\"without\",\n",
    "    \n",
    "    # --- Coordination Conjunctions (FANBOYS) ---\n",
    "    \"for\",\"and\",\"nor\",\"but\",\"or\",\"yet\",\"so\"\n",
    "}\n",
    "\n",
    "# --- CONCATENATE CORPUS ---\n",
    "corpus_text = \" \".join(df_claims[TEXT_COLUMN].dropna().astype(str))\n",
    "\n",
    "# --- CLEAN ---\n",
    "corpus_text = re.sub(r\"[^\\w\\s]\", \"\", corpus_text.lower())\n",
    "tokens = corpus_text.split()\n",
    "\n",
    "# --- FILTER ---\n",
    "filtered_tokens = [t for t in tokens if t not in STOP_WORDS]\n",
    "\n",
    "# --- COUNT ---\n",
    "word_counts = Counter(filtered_tokens)\n",
    "\n",
    "PHASE15_WORD_DF = pd.DataFrame(\n",
    "    word_counts.items(),\n",
    "    columns=[\"term\", \"count\"]\n",
    ")\n",
    "\n",
    "PHASE15_WORD_DF[\"analysis_phase\"] = \"Phase_15\"\n",
    "PHASE15_WORD_DF[\"analysis_type\"] = \"word_frequency\"\n",
    "\n",
    "PHASE15_WORD_DF = PHASE15_WORD_DF.sort_values(\"count\", ascending=False)\n",
    "\n",
    "# --- APPEND TO FINAL_DATASET ---\n",
    "FINAL_DATASET = pd.concat(\n",
    "    [FINAL_DATASET, PHASE15_WORD_DF],\n",
    "    ignore_index=True\n",
    ")\n",
    "\n",
    "print(\"Phase 15 complete: Word frequency integrated (coordination conjunctions excluded).\")\n",
    "PHASE15_WORD_DF.head(20)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "bb41a38a-90be-46d0-ae03-d439c52bb539",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Phase 16 complete: Phrase catalog integrated.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>term</th>\n",
       "      <th>count</th>\n",
       "      <th>analysis_phase</th>\n",
       "      <th>analysis_type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>dietary guidelines</td>\n",
       "      <td>98</td>\n",
       "      <td>Phase_16</td>\n",
       "      <td>bigram_phrase</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>guidelines americans</td>\n",
       "      <td>82</td>\n",
       "      <td>Phase_16</td>\n",
       "      <td>bigram_phrase</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10886</th>\n",
       "      <td>dietary guidelines americans</td>\n",
       "      <td>82</td>\n",
       "      <td>Phase_16</td>\n",
       "      <td>trigram_phrase</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2025 2030</td>\n",
       "      <td>77</td>\n",
       "      <td>Phase_16</td>\n",
       "      <td>bigram_phrase</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>americans 2025</td>\n",
       "      <td>75</td>\n",
       "      <td>Phase_16</td>\n",
       "      <td>bigram_phrase</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10887</th>\n",
       "      <td>guidelines americans 2025</td>\n",
       "      <td>75</td>\n",
       "      <td>Phase_16</td>\n",
       "      <td>trigram_phrase</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10888</th>\n",
       "      <td>americans 2025 2030</td>\n",
       "      <td>74</td>\n",
       "      <td>Phase_16</td>\n",
       "      <td>trigram_phrase</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>scientific foundation</td>\n",
       "      <td>71</td>\n",
       "      <td>Phase_16</td>\n",
       "      <td>bigram_phrase</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>foundation dietary</td>\n",
       "      <td>70</td>\n",
       "      <td>Phase_16</td>\n",
       "      <td>bigram_phrase</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10884</th>\n",
       "      <td>scientific foundation dietary</td>\n",
       "      <td>69</td>\n",
       "      <td>Phase_16</td>\n",
       "      <td>trigram_phrase</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10885</th>\n",
       "      <td>foundation dietary guidelines</td>\n",
       "      <td>69</td>\n",
       "      <td>Phase_16</td>\n",
       "      <td>trigram_phrase</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1985</th>\n",
       "      <td>highly processed</td>\n",
       "      <td>68</td>\n",
       "      <td>Phase_16</td>\n",
       "      <td>bigram_phrase</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5951</th>\n",
       "      <td>linoleic acid</td>\n",
       "      <td>66</td>\n",
       "      <td>Phase_16</td>\n",
       "      <td>bigram_phrase</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2532</th>\n",
       "      <td>processed foods</td>\n",
       "      <td>64</td>\n",
       "      <td>Phase_16</td>\n",
       "      <td>bigram_phrase</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>583</th>\n",
       "      <td>such as</td>\n",
       "      <td>56</td>\n",
       "      <td>Phase_16</td>\n",
       "      <td>bigram_phrase</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>342</th>\n",
       "      <td>saturated fat</td>\n",
       "      <td>47</td>\n",
       "      <td>Phase_16</td>\n",
       "      <td>bigram_phrase</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13721</th>\n",
       "      <td>highly processed foods</td>\n",
       "      <td>46</td>\n",
       "      <td>Phase_16</td>\n",
       "      <td>trigram_phrase</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>390</th>\n",
       "      <td>nutrient dense</td>\n",
       "      <td>38</td>\n",
       "      <td>Phase_16</td>\n",
       "      <td>bigram_phrase</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>368</th>\n",
       "      <td>added sugars</td>\n",
       "      <td>38</td>\n",
       "      <td>Phase_16</td>\n",
       "      <td>bigram_phrase</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2595</th>\n",
       "      <td>see appendix</td>\n",
       "      <td>32</td>\n",
       "      <td>Phase_16</td>\n",
       "      <td>bigram_phrase</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                term  count analysis_phase   analysis_type\n",
       "2                 dietary guidelines     98       Phase_16   bigram_phrase\n",
       "3               guidelines americans     82       Phase_16   bigram_phrase\n",
       "10886   dietary guidelines americans     82       Phase_16  trigram_phrase\n",
       "5                          2025 2030     77       Phase_16   bigram_phrase\n",
       "4                     americans 2025     75       Phase_16   bigram_phrase\n",
       "10887      guidelines americans 2025     75       Phase_16  trigram_phrase\n",
       "10888            americans 2025 2030     74       Phase_16  trigram_phrase\n",
       "0              scientific foundation     71       Phase_16   bigram_phrase\n",
       "1                 foundation dietary     70       Phase_16   bigram_phrase\n",
       "10884  scientific foundation dietary     69       Phase_16  trigram_phrase\n",
       "10885  foundation dietary guidelines     69       Phase_16  trigram_phrase\n",
       "1985                highly processed     68       Phase_16   bigram_phrase\n",
       "5951                   linoleic acid     66       Phase_16   bigram_phrase\n",
       "2532                 processed foods     64       Phase_16   bigram_phrase\n",
       "583                          such as     56       Phase_16   bigram_phrase\n",
       "342                    saturated fat     47       Phase_16   bigram_phrase\n",
       "13721         highly processed foods     46       Phase_16  trigram_phrase\n",
       "390                   nutrient dense     38       Phase_16   bigram_phrase\n",
       "368                     added sugars     38       Phase_16   bigram_phrase\n",
       "2595                    see appendix     32       Phase_16   bigram_phrase"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ==================================================\n",
    "# Phase 16 — Phrase Extraction (Bigrams + Trigrams)\n",
    "# ==================================================\n",
    "\n",
    "from collections import Counter\n",
    "from itertools import islice\n",
    "\n",
    "# --- INPUT ASSERTION ---\n",
    "if 'filtered_tokens' not in globals():\n",
    "    raise NameError(\"filtered_tokens not found. Run Phase 15 first.\")\n",
    "\n",
    "def generate_ngrams(tokens, n):\n",
    "    return zip(*(islice(tokens, i, None) for i in range(n)))\n",
    "\n",
    "# --- BIGRAMS ---\n",
    "bigrams = [\" \".join(bg) for bg in generate_ngrams(filtered_tokens, 2)]\n",
    "bigram_counts = Counter(bigrams)\n",
    "\n",
    "PHASE16_BIGRAM_DF = pd.DataFrame(\n",
    "    bigram_counts.items(),\n",
    "    columns=[\"term\", \"count\"]\n",
    ")\n",
    "\n",
    "PHASE16_BIGRAM_DF[\"analysis_phase\"] = \"Phase_16\"\n",
    "PHASE16_BIGRAM_DF[\"analysis_type\"] = \"bigram_phrase\"\n",
    "\n",
    "# --- TRIGRAMS ---\n",
    "trigrams = [\" \".join(tg) for tg in generate_ngrams(filtered_tokens, 3)]\n",
    "trigram_counts = Counter(trigrams)\n",
    "\n",
    "PHASE16_TRIGRAM_DF = pd.DataFrame(\n",
    "    trigram_counts.items(),\n",
    "    columns=[\"term\", \"count\"]\n",
    ")\n",
    "\n",
    "PHASE16_TRIGRAM_DF[\"analysis_phase\"] = \"Phase_16\"\n",
    "PHASE16_TRIGRAM_DF[\"analysis_type\"] = \"trigram_phrase\"\n",
    "\n",
    "# --- MERGE PHRASES ---\n",
    "PHASE16_PHRASE_DF = pd.concat(\n",
    "    [PHASE16_BIGRAM_DF, PHASE16_TRIGRAM_DF],\n",
    "    ignore_index=True\n",
    ").sort_values(\"count\", ascending=False)\n",
    "\n",
    "# --- APPEND TO FINAL_DATASET ---\n",
    "FINAL_DATASET = pd.concat(\n",
    "    [FINAL_DATASET, PHASE16_PHRASE_DF],\n",
    "    ignore_index=True\n",
    ")\n",
    "\n",
    "print(\"Phase 16 complete: Phrase catalog integrated.\")\n",
    "PHASE16_PHRASE_DF.head(20)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "final-export-phase15-16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ FINAL_DATASET re-exported with Phase 15 & 16 entries.\n",
      "  Total records in FINAL_DATASET: 28238\n",
      "  Analysis phases included: ['Phase_15', 'Phase_16']\n",
      "\n",
      "Exported files:\n",
      "  - final_claims_dataset.csv\n",
      "  - final_claims_dataset.json\n"
     ]
    }
   ],
   "source": [
    "# ==================================================\n",
    "# FINAL EXPORT — After Phase 15 & 16\n",
    "# ==================================================\n",
    "\n",
    "# Re-export FINAL_DATASET with Phase 15 & 16 entries included\n",
    "FINAL_DATASET.to_csv(\"final_claims_dataset.csv\", index=False)\n",
    "FINAL_DATASET.to_json(\"final_claims_dataset.json\", orient=\"records\")\n",
    "\n",
    "print(\"✓ FINAL_DATASET re-exported with Phase 15 & 16 entries.\")\n",
    "print(f\"  Total records in FINAL_DATASET: {len(FINAL_DATASET)}\")\n",
    "\n",
    "# Get unique phases, excluding NaN values\n",
    "phases = [p for p in FINAL_DATASET['analysis_phase'].unique() if pd.notna(p)]\n",
    "phases_sorted = sorted(phases)\n",
    "print(f\"  Analysis phases included: {phases_sorted}\")\n",
    "print(f\"\\nExported files:\")\n",
    "print(f\"  - final_claims_dataset.csv\")\n",
    "print(f\"  - final_claims_dataset.json\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e5dd2725-9854-4b30-89d1-0716b0c5934d",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert \"Phase_15\" in FINAL_DATASET[\"analysis_phase\"].values\n",
    "assert \"Phase_16\" in FINAL_DATASET[\"analysis_phase\"].values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "78613fc3-313e-48ba-9fa0-4675123430cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['claim_id', 'claim_text', 'page', 'section', 'excluded', 'claim_type', 'falsifiability', 'evidence_type', 'regulatory_relevance']\n"
     ]
    }
   ],
   "source": [
    "print(df_claims.columns.tolist())\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
